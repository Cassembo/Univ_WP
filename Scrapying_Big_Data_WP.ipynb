{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJVIgI+0+tgbzz93PlEbNZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cassembo/Univ_WP/blob/main/Scrapying_Big_Data_WP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1W3oHC5MLXz",
        "outputId": "d348e3ab-9920-4031-87c4-98dac91b19aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to the site.\n",
            "Sample Data:\n",
            "Titles: ['N/A', 'N/A', 'N/A']\n",
            "Authors: ['N/A', 'N/A', 'N/A']\n",
            "Ratings: ['N/A', 'N/A', 'N/A']\n",
            "Genres: ['N/A', 'N/A', 'N/A']\n",
            "Scraping completed. Data saved to 'career_growth_books.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# URL to scrape\n",
        "url = \"https://www.everand.com/books/Career-Growth\"\n",
        "\n",
        "# Send a GET request to the website\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully connected to the site.\")\n",
        "else:\n",
        "    print(f\"Failed to connect to the site. Status code: {response.status_code}\")\n",
        "    exit()\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Create lists to store scraped data\n",
        "titles = []\n",
        "authors = []\n",
        "ratings = []\n",
        "genres = []\n",
        "\n",
        "# Locate the main container holding the books\n",
        "books = soup.find_all(\"div\", class_=\"\")\n",
        "\n",
        "if not books:\n",
        "    print(\"No books found. Please verify the structure of the webpage.\")\n",
        "    exit()\n",
        "\n",
        "for book in books:\n",
        "    # Extract title\n",
        "    title_tag = book.find(\"h3\", class_=\"Metadata-module_heading__ebykj Metadata-module_title__zZtUI\")\n",
        "    title = title_tag.text.strip() if title_tag else \"N/A\"\n",
        "    titles.append(title)\n",
        "\n",
        "    # Extract author\n",
        "    author_tag = book.find(\"a\", class_=\"author\")\n",
        "    author = author_tag.text.replace(\"Author:\", \"\").strip() if author_tag else \"N/A\"\n",
        "    authors.append(author)\n",
        "\n",
        "    # Extract rating\n",
        "    rating_tag = book.find(\"span\", class_=\"book-rating\")\n",
        "    rating = rating_tag.text.strip() if rating_tag else \"N/A\"\n",
        "    ratings.append(rating)\n",
        "\n",
        "    # Extract genre\n",
        "    genre_tag = book.find(\"p\", class_=\"book-genre\")\n",
        "    genre = genre_tag.text.replace(\"Genre:\", \"\").strip() if genre_tag else \"N/A\"\n",
        "    genres.append(genre)\n",
        "\n",
        "# Debugging: Print the first few extracted entries\n",
        "print(\"Sample Data:\")\n",
        "print(\"Titles:\", titles[:3])\n",
        "print(\"Authors:\", authors[:3])\n",
        "print(\"Ratings:\", ratings[:3])\n",
        "print(\"Genres:\", genres[:3])\n",
        "\n",
        "# Create a DataFrame to store the scraped data\n",
        "data = {\n",
        "    \"Title\": titles,\n",
        "    \"Author\": authors,\n",
        "    \"Rating\": ratings,\n",
        "    \"Genre\": genres\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "if not df.empty:\n",
        "    df.to_csv(\"career_growth_books.csv\", index=False)\n",
        "    print(\"Scraping completed. Data saved to 'career_growth_books.csv'.\")\n",
        "else:\n",
        "    print(\"No data to save. Please check the webpage structure or selectors.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd  # Import pandas for saving to CSV\n",
        "\n",
        "# Base URL for scraping book data\n",
        "base_url = 'https://books.toscrape.com/catalogue/page-'\n",
        "\n",
        "# Function to scrape book data from a specific page\n",
        "def scrape_books(page_number):\n",
        "    # Construct the full URL\n",
        "    url = f'{base_url}{page_number}.html'\n",
        "\n",
        "    # Send a GET request to the website\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code != 200:\n",
        "        print(f'Failed to retrieve the webpage for page {page_number}: {response.status_code}')\n",
        "        return []\n",
        "\n",
        "    # Parse the HTML content of the page\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all books on the page\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    book_data = []\n",
        "\n",
        "    # Loop through each book and extract its details\n",
        "    for book in books:\n",
        "        title = book.find('h3').find('a')['title']\n",
        "\n",
        "        # Handle cases where the author is not found (e.g., some entries might lack author info)\n",
        "        author = book.find('p', class_='author')\n",
        "        author = author.text if author else 'Unknown'  # Default to 'Unknown' if no author is found\n",
        "\n",
        "        # Handle cases where the rating is not found\n",
        "        rating = book.find('p', class_='star-rating')\n",
        "        rating = rating['class'][1] if rating else 'No rating'  # Default to 'No rating' if no rating is found\n",
        "\n",
        "        # Genre is not part of this page's structure, so we may skip it\n",
        "        genre = 'N/A'  # No genre information in this example\n",
        "\n",
        "        # Append book details to the list\n",
        "        book_data.append([title, author, rating, genre])\n",
        "\n",
        "    return book_data\n",
        "\n",
        "# Scrape pages and save the book data to a CSV\n",
        "all_books = []\n",
        "\n",
        "# Scrape all 100 pages (there are only 100 pages on \"Books to Scrape\")\n",
        "for page in range(1, 101):  # Loop through all 100 pages\n",
        "    print(f'Scraping page {page}...')  # Print the current page number\n",
        "    books = scrape_books(page)\n",
        "    all_books.extend(books)\n",
        "\n",
        "# Create a DataFrame and save to CSV\n",
        "df = pd.DataFrame(all_books, columns=['Title', 'Author', 'Rating', 'Genre'])\n",
        "\n",
        "# Save to a CSV file\n",
        "csv_filename = 'books_data.csv'\n",
        "df.to_csv(csv_filename, index=False)\n",
        "\n",
        "print(f'Successfully saved book data to {csv_filename}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvzhYZ74aloU",
        "outputId": "93ae1679-51b1-4553-f610-c03875727ed2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Failed to retrieve the webpage for page 51: 404\n",
            "Scraping page 52...\n",
            "Failed to retrieve the webpage for page 52: 404\n",
            "Scraping page 53...\n",
            "Failed to retrieve the webpage for page 53: 404\n",
            "Scraping page 54...\n",
            "Failed to retrieve the webpage for page 54: 404\n",
            "Scraping page 55...\n",
            "Failed to retrieve the webpage for page 55: 404\n",
            "Scraping page 56...\n",
            "Failed to retrieve the webpage for page 56: 404\n",
            "Scraping page 57...\n",
            "Failed to retrieve the webpage for page 57: 404\n",
            "Scraping page 58...\n",
            "Failed to retrieve the webpage for page 58: 404\n",
            "Scraping page 59...\n",
            "Failed to retrieve the webpage for page 59: 404\n",
            "Scraping page 60...\n",
            "Failed to retrieve the webpage for page 60: 404\n",
            "Scraping page 61...\n",
            "Failed to retrieve the webpage for page 61: 404\n",
            "Scraping page 62...\n",
            "Failed to retrieve the webpage for page 62: 404\n",
            "Scraping page 63...\n",
            "Failed to retrieve the webpage for page 63: 404\n",
            "Scraping page 64...\n",
            "Failed to retrieve the webpage for page 64: 404\n",
            "Scraping page 65...\n",
            "Failed to retrieve the webpage for page 65: 404\n",
            "Scraping page 66...\n",
            "Failed to retrieve the webpage for page 66: 404\n",
            "Scraping page 67...\n",
            "Failed to retrieve the webpage for page 67: 404\n",
            "Scraping page 68...\n",
            "Failed to retrieve the webpage for page 68: 404\n",
            "Scraping page 69...\n",
            "Failed to retrieve the webpage for page 69: 404\n",
            "Scraping page 70...\n",
            "Failed to retrieve the webpage for page 70: 404\n",
            "Scraping page 71...\n",
            "Failed to retrieve the webpage for page 71: 404\n",
            "Scraping page 72...\n",
            "Failed to retrieve the webpage for page 72: 404\n",
            "Scraping page 73...\n",
            "Failed to retrieve the webpage for page 73: 404\n",
            "Scraping page 74...\n",
            "Failed to retrieve the webpage for page 74: 404\n",
            "Scraping page 75...\n",
            "Failed to retrieve the webpage for page 75: 404\n",
            "Scraping page 76...\n",
            "Failed to retrieve the webpage for page 76: 404\n",
            "Scraping page 77...\n",
            "Failed to retrieve the webpage for page 77: 404\n",
            "Scraping page 78...\n",
            "Failed to retrieve the webpage for page 78: 404\n",
            "Scraping page 79...\n",
            "Failed to retrieve the webpage for page 79: 404\n",
            "Scraping page 80...\n",
            "Failed to retrieve the webpage for page 80: 404\n",
            "Scraping page 81...\n",
            "Failed to retrieve the webpage for page 81: 404\n",
            "Scraping page 82...\n",
            "Failed to retrieve the webpage for page 82: 404\n",
            "Scraping page 83...\n",
            "Failed to retrieve the webpage for page 83: 404\n",
            "Scraping page 84...\n",
            "Failed to retrieve the webpage for page 84: 404\n",
            "Scraping page 85...\n",
            "Failed to retrieve the webpage for page 85: 404\n",
            "Scraping page 86...\n",
            "Failed to retrieve the webpage for page 86: 404\n",
            "Scraping page 87...\n",
            "Failed to retrieve the webpage for page 87: 404\n",
            "Scraping page 88...\n",
            "Failed to retrieve the webpage for page 88: 404\n",
            "Scraping page 89...\n",
            "Failed to retrieve the webpage for page 89: 404\n",
            "Scraping page 90...\n",
            "Failed to retrieve the webpage for page 90: 404\n",
            "Scraping page 91...\n",
            "Failed to retrieve the webpage for page 91: 404\n",
            "Scraping page 92...\n",
            "Failed to retrieve the webpage for page 92: 404\n",
            "Scraping page 93...\n",
            "Failed to retrieve the webpage for page 93: 404\n",
            "Scraping page 94...\n",
            "Failed to retrieve the webpage for page 94: 404\n",
            "Scraping page 95...\n",
            "Failed to retrieve the webpage for page 95: 404\n",
            "Scraping page 96...\n",
            "Failed to retrieve the webpage for page 96: 404\n",
            "Scraping page 97...\n",
            "Failed to retrieve the webpage for page 97: 404\n",
            "Scraping page 98...\n",
            "Failed to retrieve the webpage for page 98: 404\n",
            "Scraping page 99...\n",
            "Failed to retrieve the webpage for page 99: 404\n",
            "Scraping page 100...\n",
            "Failed to retrieve the webpage for page 100: 404\n",
            "Successfully saved book data to books_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Scrape Books to Scrape website\n",
        "def scrape_books_to_scrape(page_number):\n",
        "    base_url = 'https://books.toscrape.com/catalogue/page-'\n",
        "    url = f'{base_url}{page_number}.html'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f'Failed to retrieve page {page_number}')\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "    book_data = []\n",
        "\n",
        "    for book in books:\n",
        "        title = book.find('h3').find('a')['title']\n",
        "        author = book.find('p', class_='author').text if book.find('p', class_='author') else 'Unknown'\n",
        "        rating = book.find('p', class_='star-rating')['class'][1] if book.find('p', class_='star-rating') else 'No rating'\n",
        "        genre = 'N/A'\n",
        "        book_data.append([title, author, rating, genre])\n",
        "\n",
        "    return book_data\n",
        "\n",
        "# Scrape eBay website\n",
        "def scrape_ebay(page_number):\n",
        "    base_url = 'https://www.ebay.com/sch/i.html?_nkw=laptop&_sop=12&_pgn='\n",
        "    url = f'{base_url}{page_number}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f'Failed to retrieve eBay page {page_number}')\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    items = soup.find_all('li', class_='s-item')\n",
        "    item_data = []\n",
        "\n",
        "    for item in items:\n",
        "        title = item.find('h3', class_='s-item__title').text if item.find('h3', class_='s-item__title') else 'No Title'\n",
        "        price = item.find('span', class_='s-item__price').text if item.find('span', class_='s-item__price') else 'No Price'\n",
        "        shipping = item.find('span', class_='s-item__shipping').text if item.find('span', class_='s-item__shipping') else 'No Shipping Info'\n",
        "        item_data.append([title, price, shipping])\n",
        "\n",
        "    return item_data\n",
        "\n",
        "# Scrape AliExpress website (new code for AliExpress)\n",
        "def scrape_aliexpress(page_number):\n",
        "    base_url = 'https://www.aliexpress.com/w/wholesale-laptop.html?spm=a2g0o.search0104.0.0.26092c5cTdpRiA&page='\n",
        "    url = f'{base_url}{page_number}'\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f'Failed to retrieve AliExpress page {page_number}')\n",
        "        return []\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    products = soup.find_all('div', class_='item')\n",
        "    product_data = []\n",
        "\n",
        "    for product in products:\n",
        "        title = product.find('a', class_='item-title').text if product.find('a', class_='item-title') else 'No Title'\n",
        "        price = product.find('span', class_='price-current').text if product.find('span', class_='price-current') else 'No Price'\n",
        "        shipping = product.find('span', class_='shipping').text if product.find('span', class_='shipping') else 'No Shipping Info'\n",
        "        product_data.append([title, price, shipping])\n",
        "\n",
        "    return product_data\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# ... (Your existing scraping functions) ...\n",
        "\n",
        "# Combine data from multiple sources\n",
        "def scrape_multiple_sources():\n",
        "    all_data = []\n",
        "\n",
        "    # Scrape Books to Scrape (pages 1-5 for demo)\n",
        "    for page in range(1, 51):\n",
        "        print(f'Scraping Books to Scrape page {page}...')\n",
        "        all_data.extend(scrape_books_to_scrape(page))\n",
        "\n",
        "    # Scrape eBay (pages 1-5 for demo)\n",
        "    for page in range(1, 60):\n",
        "        print(f'Scraping eBay page {page}...')\n",
        "        all_data.extend(scrape_ebay(page))\n",
        "\n",
        "    # Scrape AliExpress (pages 1-5 for demo)\n",
        "    for page in range(1, 60):\n",
        "        print(f'Scraping AliExpress page {page}...')\n",
        "        all_data.extend(scrape_aliexpress(page))\n",
        "\n",
        "    return all_data\n",
        "\n",
        "# Collect data\n",
        "data = scrape_multiple_sources()\n",
        "\n",
        "# Save to a CSV file\n",
        "# Adjust column names to match the data structure\n",
        "df = pd.DataFrame(data, columns=['Title', 'Author/Price', 'Rating/Shipping', 'Genre'])\n",
        "df.to_csv('combined_data.csv', index=False)\n",
        "\n",
        "print('Successfully saved combined data to combined_data.csv')\n",
        "\n",
        "print('Successfully saved combined data to combined_data.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13UreNOacZii",
        "outputId": "c77276c4-b0bc-4a52-b1c4-2dc3d77986e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping Books to Scrape page 1...\n",
            "Scraping Books to Scrape page 2...\n",
            "Scraping Books to Scrape page 3...\n",
            "Scraping Books to Scrape page 4...\n",
            "Scraping Books to Scrape page 5...\n",
            "Scraping Books to Scrape page 6...\n",
            "Scraping Books to Scrape page 7...\n",
            "Scraping Books to Scrape page 8...\n",
            "Scraping Books to Scrape page 9...\n",
            "Scraping Books to Scrape page 10...\n",
            "Scraping Books to Scrape page 11...\n",
            "Scraping Books to Scrape page 12...\n",
            "Scraping Books to Scrape page 13...\n",
            "Scraping Books to Scrape page 14...\n",
            "Scraping Books to Scrape page 15...\n",
            "Scraping Books to Scrape page 16...\n",
            "Scraping Books to Scrape page 17...\n",
            "Scraping Books to Scrape page 18...\n",
            "Scraping Books to Scrape page 19...\n",
            "Scraping Books to Scrape page 20...\n",
            "Scraping Books to Scrape page 21...\n",
            "Scraping Books to Scrape page 22...\n",
            "Scraping Books to Scrape page 23...\n",
            "Scraping Books to Scrape page 24...\n",
            "Scraping Books to Scrape page 25...\n",
            "Scraping Books to Scrape page 26...\n",
            "Scraping Books to Scrape page 27...\n",
            "Scraping Books to Scrape page 28...\n",
            "Scraping Books to Scrape page 29...\n",
            "Scraping Books to Scrape page 30...\n",
            "Scraping Books to Scrape page 31...\n",
            "Scraping Books to Scrape page 32...\n",
            "Scraping Books to Scrape page 33...\n",
            "Scraping Books to Scrape page 34...\n",
            "Scraping Books to Scrape page 35...\n",
            "Scraping Books to Scrape page 36...\n",
            "Scraping Books to Scrape page 37...\n",
            "Scraping Books to Scrape page 38...\n",
            "Scraping Books to Scrape page 39...\n",
            "Scraping Books to Scrape page 40...\n",
            "Scraping Books to Scrape page 41...\n",
            "Scraping Books to Scrape page 42...\n",
            "Scraping Books to Scrape page 43...\n",
            "Scraping Books to Scrape page 44...\n",
            "Scraping Books to Scrape page 45...\n",
            "Scraping Books to Scrape page 46...\n",
            "Scraping Books to Scrape page 47...\n",
            "Scraping Books to Scrape page 48...\n",
            "Scraping Books to Scrape page 49...\n",
            "Scraping Books to Scrape page 50...\n",
            "Scraping eBay page 1...\n",
            "Scraping eBay page 2...\n",
            "Scraping eBay page 3...\n",
            "Scraping eBay page 4...\n",
            "Scraping eBay page 5...\n",
            "Scraping eBay page 6...\n",
            "Scraping eBay page 7...\n",
            "Scraping eBay page 8...\n",
            "Scraping eBay page 9...\n",
            "Scraping eBay page 10...\n",
            "Scraping eBay page 11...\n",
            "Scraping eBay page 12...\n",
            "Scraping eBay page 13...\n",
            "Scraping eBay page 14...\n",
            "Scraping eBay page 15...\n",
            "Scraping eBay page 16...\n",
            "Scraping eBay page 17...\n",
            "Scraping eBay page 18...\n",
            "Scraping eBay page 19...\n",
            "Scraping eBay page 20...\n",
            "Scraping eBay page 21...\n",
            "Scraping eBay page 22...\n",
            "Scraping eBay page 23...\n",
            "Scraping eBay page 24...\n",
            "Scraping eBay page 25...\n",
            "Scraping eBay page 26...\n",
            "Scraping eBay page 27...\n",
            "Scraping eBay page 28...\n",
            "Scraping eBay page 29...\n",
            "Scraping eBay page 30...\n",
            "Scraping eBay page 31...\n",
            "Scraping eBay page 32...\n",
            "Scraping eBay page 33...\n",
            "Scraping eBay page 34...\n",
            "Scraping eBay page 35...\n",
            "Scraping eBay page 36...\n",
            "Scraping eBay page 37...\n",
            "Scraping eBay page 38...\n",
            "Scraping eBay page 39...\n",
            "Scraping eBay page 40...\n",
            "Scraping eBay page 41...\n",
            "Scraping eBay page 42...\n",
            "Scraping eBay page 43...\n",
            "Scraping eBay page 44...\n",
            "Scraping eBay page 45...\n",
            "Scraping eBay page 46...\n",
            "Scraping eBay page 47...\n",
            "Scraping eBay page 48...\n",
            "Scraping eBay page 49...\n",
            "Scraping eBay page 50...\n",
            "Scraping eBay page 51...\n",
            "Scraping eBay page 52...\n",
            "Scraping eBay page 53...\n",
            "Scraping eBay page 54...\n",
            "Scraping eBay page 55...\n",
            "Scraping eBay page 56...\n",
            "Scraping eBay page 57...\n",
            "Scraping eBay page 58...\n",
            "Scraping eBay page 59...\n",
            "Scraping AliExpress page 1...\n",
            "Scraping AliExpress page 2...\n",
            "Scraping AliExpress page 3...\n",
            "Scraping AliExpress page 4...\n",
            "Scraping AliExpress page 5...\n",
            "Scraping AliExpress page 6...\n",
            "Scraping AliExpress page 7...\n",
            "Scraping AliExpress page 8...\n",
            "Scraping AliExpress page 9...\n",
            "Scraping AliExpress page 10...\n",
            "Scraping AliExpress page 11...\n",
            "Scraping AliExpress page 12...\n",
            "Scraping AliExpress page 13...\n",
            "Scraping AliExpress page 14...\n",
            "Scraping AliExpress page 15...\n",
            "Scraping AliExpress page 16...\n",
            "Scraping AliExpress page 17...\n",
            "Scraping AliExpress page 18...\n",
            "Scraping AliExpress page 19...\n",
            "Scraping AliExpress page 20...\n",
            "Scraping AliExpress page 21...\n",
            "Scraping AliExpress page 22...\n",
            "Scraping AliExpress page 23...\n",
            "Scraping AliExpress page 24...\n",
            "Scraping AliExpress page 25...\n",
            "Scraping AliExpress page 26...\n",
            "Scraping AliExpress page 27...\n",
            "Scraping AliExpress page 28...\n",
            "Scraping AliExpress page 29...\n",
            "Scraping AliExpress page 30...\n",
            "Scraping AliExpress page 31...\n",
            "Scraping AliExpress page 32...\n",
            "Scraping AliExpress page 33...\n",
            "Scraping AliExpress page 34...\n",
            "Scraping AliExpress page 35...\n",
            "Scraping AliExpress page 36...\n",
            "Scraping AliExpress page 37...\n",
            "Scraping AliExpress page 38...\n",
            "Scraping AliExpress page 39...\n",
            "Scraping AliExpress page 40...\n",
            "Scraping AliExpress page 41...\n",
            "Scraping AliExpress page 42...\n",
            "Scraping AliExpress page 43...\n",
            "Scraping AliExpress page 44...\n",
            "Scraping AliExpress page 45...\n",
            "Scraping AliExpress page 46...\n",
            "Scraping AliExpress page 47...\n",
            "Scraping AliExpress page 48...\n",
            "Scraping AliExpress page 49...\n",
            "Scraping AliExpress page 50...\n",
            "Scraping AliExpress page 51...\n",
            "Scraping AliExpress page 52...\n",
            "Scraping AliExpress page 53...\n",
            "Scraping AliExpress page 54...\n",
            "Scraping AliExpress page 55...\n",
            "Scraping AliExpress page 56...\n",
            "Scraping AliExpress page 57...\n",
            "Scraping AliExpress page 58...\n",
            "Scraping AliExpress page 59...\n",
            "Successfully saved combined data to combined_data.csv\n",
            "Successfully saved combined data to combined_data.csv\n"
          ]
        }
      ]
    }
  ]
}